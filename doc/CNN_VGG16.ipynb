{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CNN_VGG16.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "3h2aY1Dl0J_a"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import pairwise_distances\n",
        "import time\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras import layers"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rose-1rCjqGA",
        "outputId": "d860ff0e-e1da-40fe-df51-15acc26ec9dd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from google.colab import drive\n",
        "import os\n",
        "drive.mount('/content/drive')\n",
        "os.chdir(\"/content/drive/My Drive\")"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jwbMNjRu4-M6"
      },
      "source": [
        "label = pd.read_pickle('label_full.pkl')"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_H5e9GC4eokm",
        "outputId": "5395757a-232a-4692-89e1-3f01013311a9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "photos = []\n",
        "folder = 'train_set/images/'\n",
        "# enumerate files in the directory\n",
        "for file in os.listdir(folder):\n",
        "    photo = tf.keras.preprocessing.image.load_img(folder + file, target_size = (96,96))\n",
        "    photo = tf.keras.preprocessing.image.img_to_array(photo)\n",
        "    photos.append(photo)\n",
        "# convert to numpy arrays\n",
        "photos = np.asarray(photos)\n",
        "labels = np.asarray(label)\n",
        "print(photos.shape, labels.shape)\n",
        "# save the reshaped photos\n",
        "# save('resized_image.npy',photos)"
      ],
      "execution_count": 105,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(3000, 96, 96, 3) (3000,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nLL95hxU_V8C"
      },
      "source": [
        "# split data into train and test set\n",
        "X_train, X_test, y_train, y_test = train_test_split(photos, labels, test_size=0.2, random_state=42)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=66)\n",
        "\n",
        "bool_train_labels = y_train != 0\n",
        "\n",
        "# form np arrays of labels\n",
        "y_train = np.array(y_train)\n",
        "y_test = np.array(y_test)\n",
        "y_val = np.array(y_val)"
      ],
      "execution_count": 106,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1GVEtJVg2XDS",
        "outputId": "35aad14d-8d75-41d8-8649-83bd4ae92449",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Create convolutional base\n",
        "prior = keras.applications.VGG16(\n",
        "    include_top=False, \n",
        "    weights='imagenet',\n",
        "    input_shape=(96,96, 3)\n",
        ")\n",
        "model = Sequential()\n",
        "model.add(prior)\n",
        "model.add(layers.Conv2D(64, kernel_size=(3, 3), input_shape=(96, 96, 3), activation='relu', padding='same'))\n",
        "model.add(layers.Conv2D(64, kernel_size=(3, 3), input_shape=(96, 96, 3), activation='relu', padding='same'))\n",
        "model.add(layers.MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(layers.Flatten())\n",
        "model.add(layers.Dense(32, activation='relu', name='Dense_Intermediate'))\n",
        "model.add(layers.Dropout(0.1, name='Dropout_Regularization'))\n",
        "model.add(layers.Dense(1, activation='sigmoid', name='Output'))\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": 126,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_12\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "vgg16 (Functional)           (None, 3, 3, 512)         14714688  \n",
            "_________________________________________________________________\n",
            "conv2d_4 (Conv2D)            (None, 3, 3, 64)          294976    \n",
            "_________________________________________________________________\n",
            "conv2d_5 (Conv2D)            (None, 3, 3, 64)          36928     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_2 (MaxPooling2 (None, 1, 1, 64)          0         \n",
            "_________________________________________________________________\n",
            "flatten_10 (Flatten)         (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "Dense_Intermediate (Dense)   (None, 32)                2080      \n",
            "_________________________________________________________________\n",
            "Dropout_Regularization (Drop (None, 32)                0         \n",
            "_________________________________________________________________\n",
            "Output (Dense)               (None, 1)                 33        \n",
            "=================================================================\n",
            "Total params: 15,048,705\n",
            "Trainable params: 15,048,705\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "71z3Dprl7foh"
      },
      "source": [
        "# Freeze the VGG16 model, e.g. do not train any of its weights.\n",
        "# We will just use it as-is.\n",
        "for cnn_block_layer in (model.layers)[:19]:\n",
        "    cnn_block_layer.trainable = False\n",
        "\n",
        "\n",
        "METRICS = [\n",
        "           keras.metrics.BinaryAccuracy(name='accuracy'),\n",
        "           keras.metrics.AUC(name='auc'),\n",
        "           keras.metrics.Precision(name='precision'),\n",
        "           keras.metrics.Recall(name='recall')\n",
        "]\n",
        "\n",
        "model.compile(\n",
        "    optimizer='adam',\n",
        "              loss=tf.keras.losses.BinaryCrossentropy(),\n",
        "              metrics= METRICS\n",
        ")"
      ],
      "execution_count": 127,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KnEkquKo8J1u"
      },
      "source": [
        "# Finally we fit the model. I use two callbacks here: EarlyStopping,\n",
        "# which stops the model short of its full 20 epochs if validation \n",
        "# performance consistently gets worse; and ReduceLROnPlateau, which \n",
        "# reduces the learning rate 10x at a time when it detects model \n",
        "# performance is no longer improving between epochs.\n",
        "#\n",
        "\n",
        "early_stopping = tf.keras.callbacks.EarlyStopping(\n",
        "    monitor='val_auc', \n",
        "    verbose=1,\n",
        "    patience=5,\n",
        "    mode='max',\n",
        "    restore_best_weights=True)\n",
        "\n",
        "# Recall that our dataset is highly imbalanced. We deal with this\n",
        "# problem by generating class weights and passing them to the model\n",
        "# at training time. The model will use the class weights to adjust\n",
        "# how it trains so that each class is considered equally important to\n",
        "# get right, even if the actual distribution of images is highly \n",
        "# variable.\n",
        "from sklearn.utils import class_weight\n",
        "class_weights = class_weight.compute_class_weight('balanced',\n",
        "                                                 np.unique(y_train),\n",
        "                                                 y_train) \n",
        "class_weight = {0: class_weights[0], 1: class_weights[1]}\n",
        "\n",
        "# I found that a batch size of 128 offers the best trade-off between\n",
        "# model training time and batch volatility.\n",
        "\n",
        "batch_size = 128\n"
      ],
      "execution_count": 128,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KH8bwbHAA2Wb",
        "outputId": "353125b5-abc1-44be-9f47-f4f26f7b5d18",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# fit the model.\n",
        "\n",
        "model.fit(X_train, y_train, \n",
        "          steps_per_epoch=len(X_train) // batch_size,\n",
        "          validation_data=(X_val, y_val),\n",
        "          epochs=20,\n",
        "          validation_steps=len(X_val) // batch_size,\n",
        "          class_weight=class_weight,\n",
        "          callbacks=[early_stopping,\n",
        "          tf.keras.callbacks.ReduceLROnPlateau(patience=2)]\n",
        "    )"
      ],
      "execution_count": 129,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "15/15 [==============================] - 210s 14s/step - loss: 4.8951 - accuracy: 0.7609 - auc: 0.4965 - precision: 0.2013 - recall: 0.0845 - val_loss: 1.9569 - val_accuracy: 0.7833 - val_auc: 0.4795 - val_precision: 0.1667 - val_recall: 0.0326\n",
            "Epoch 2/20\n",
            "15/15 [==============================] - 205s 14s/step - loss: 4.9213 - accuracy: 0.7510 - auc: 0.5005 - precision: 0.1636 - recall: 0.0736 - val_loss: 1.9569 - val_accuracy: 0.7833 - val_auc: 0.4795 - val_precision: 0.1667 - val_recall: 0.0326\n",
            "Epoch 3/20\n",
            "15/15 [==============================] - 205s 14s/step - loss: 4.9786 - accuracy: 0.7745 - auc: 0.4914 - precision: 0.2381 - recall: 0.0817 - val_loss: 1.9569 - val_accuracy: 0.7833 - val_auc: 0.4795 - val_precision: 0.1667 - val_recall: 0.0326\n",
            "Epoch 4/20\n",
            "15/15 [==============================] - 208s 14s/step - loss: 4.9153 - accuracy: 0.7661 - auc: 0.5060 - precision: 0.1746 - recall: 0.0599 - val_loss: 1.9569 - val_accuracy: 0.7833 - val_auc: 0.4795 - val_precision: 0.1667 - val_recall: 0.0326\n",
            "Epoch 5/20\n",
            "15/15 [==============================] - 205s 14s/step - loss: 4.9891 - accuracy: 0.7625 - auc: 0.4923 - precision: 0.1931 - recall: 0.0763 - val_loss: 1.9569 - val_accuracy: 0.7833 - val_auc: 0.4795 - val_precision: 0.1667 - val_recall: 0.0326\n",
            "Epoch 6/20\n",
            "15/15 [==============================] - ETA: 0s - loss: 4.8583 - accuracy: 0.7651 - auc: 0.5047 - precision: 0.2123 - recall: 0.0845 Restoring model weights from the end of the best epoch.\n",
            "15/15 [==============================] - 205s 14s/step - loss: 4.8583 - accuracy: 0.7651 - auc: 0.5047 - precision: 0.2123 - recall: 0.0845 - val_loss: 1.9569 - val_accuracy: 0.7833 - val_auc: 0.4795 - val_precision: 0.1667 - val_recall: 0.0326\n",
            "Epoch 00006: early stopping\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f9e41473390>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 129
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_6zmw1icRbhY",
        "outputId": "38471978-4852-4773-93b7-e12c1db0d557",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# on test set\n",
        "pred_bal = model.evaluate(X_test, y_test)\n",
        "print('Loss on balanced test set is {:.2f}'.format(pred_bal[0]))\n",
        "print('Accuracy on balanced test set is {:.2f}'.format(pred_bal[1]))\n",
        "print('AUC on balanced test set is {:.2f}'.format(pred_bal[2]))"
      ],
      "execution_count": 130,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "19/19 [==============================] - 53s 3s/step - loss: 2.0919 - accuracy: 0.7450 - auc: 0.5326 - precision: 0.2308 - recall: 0.0432\n",
            "Loss on balanced test set is 2.09\n",
            "Accuracy on balanced test set is 0.75\n",
            "AUC on balanced test set is 0.53\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}