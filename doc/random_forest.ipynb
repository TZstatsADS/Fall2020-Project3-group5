{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import RandomizedSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/IPython/core/interactiveshell.py:3254: FutureWarning: arrays to stack must be passed as a \"sequence\" type such as list or tuple. Support for non-sequence iterables such as generators is deprecated as of NumPy 1.16 and will raise an error in the future.\n",
      "  if (await self.run_code(code, result,  async_=asy)):\n",
      "/opt/anaconda3/lib/python3.7/site-packages/IPython/core/interactiveshell.py:3254: FutureWarning: arrays to stack must be passed as a \"sequence\" type such as list or tuple. Support for non-sequence iterables such as generators is deprecated as of NumPy 1.16 and will raise an error in the future.\n",
      "  if (await self.run_code(code, result,  async_=asy)):\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "# loading data and features:\n",
    "\n",
    "fiducial_pt_full = pd.read_pickle('../output/fiducial_pt_full.pkl')\n",
    "label_full = pd.read_pickle('../output/label_full.pkl')\n",
    "\n",
    "# Randomly splitting data into training & test sets:\n",
    "RSEED = 42\n",
    "\n",
    "x_train, x_test, train_labels, test_labels = train_test_split(fiducial_pt_full, \n",
    "                                                          label_full, \n",
    "                                                          test_size=0.2, \n",
    "                                                          random_state=RSEED)\n",
    "\n",
    "# Using pairwise distance as feature.\n",
    "# extracting pairwise distance as features (78*77/2=3303 features)\n",
    "# nrow=number of records of the dataset; ncol=3303\n",
    "\n",
    "feature_train = np.stack((metrics.pairwise_distances(x_train[i])[np.triu_indices(78, k = 1)] for i in range(x_train.shape[0])))\n",
    "feature_test = np.stack((metrics.pairwise_distances(x_test[i])[np.triu_indices(78, k = 1)] for i in range(x_test.shape[0])))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training takes 8.38\n"
     ]
    }
   ],
   "source": [
    "model = RandomForestClassifier(n_estimators=100, \n",
    "                               bootstrap = True,\n",
    "                               max_features = 'sqrt')\n",
    "start = time.time()\n",
    "# Fit on training data\n",
    "model.fit(feature_train, train_labels)\n",
    "print('Training takes {:.2f}'.format(time.time()-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing takes 0.23\n"
     ]
    }
   ],
   "source": [
    "# Make probability predictions\n",
    "start = time.time()\n",
    "\n",
    "train_probs = model.predict_proba(feature_train)[:, 1]\n",
    "probs = model.predict_proba(feature_test)[:, 1]\n",
    "\n",
    "train_predictions = model.predict(feature_train)\n",
    "predictions = model.predict(feature_test)\n",
    "\n",
    "print('Testing takes {:.2f}'.format(time.time()-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train ROC AUC Score: 0.9999999999999999\n",
      "Test ROC AUC  Score: 0.8125594968710498\n"
     ]
    }
   ],
   "source": [
    "print(f'Train ROC AUC Score: {metrics.roc_auc_score(train_labels, train_probs)}')\n",
    "print(f'Test ROC AUC  Score: {metrics.roc_auc_score(test_labels, probs)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, roc_auc_score, accuracy_score\n",
    "def clf_metrics(y_true, y_pred, y_score):\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    \n",
    "    # reweight dataset in order to estimate the accuracy of \"balanced\" data set\n",
    "    weight_data = np.zeros(len(y_true))\n",
    "    for v in np.unique(y_true):\n",
    "        weight_data[y_true==v] = 0.5*len(y_true)/np.sum(y_true==v)\n",
    "    weighted_acc = np.sum(weight_data * (y_pred==y_true)/np.sum(weight_data))\n",
    "    \n",
    "    precision = precision_score(y_true, y_pred)\n",
    "    recall = recall_score(y_true, y_pred)\n",
    "    auc = roc_auc_score(y_true, y_score, average='weighted')\n",
    "    \n",
    "    df = pd.DataFrame({'accuracy':[accuracy],'weighted acc':[weighted_acc],\n",
    "                       'precision': [precision], 'recall': [recall], 'auc':[auc]})\n",
    "    print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   accuracy  weighted acc  precision    recall       auc\n",
      "0  0.801667       0.57948   0.884615  0.165468  0.812559\n"
     ]
    }
   ],
   "source": [
    "clf_metrics(test_labels, predictions, probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run time is 15.57 s\n"
     ]
    }
   ],
   "source": [
    "end_time = time.time()\n",
    "print('Run time is {:.2f} s'.format(end_time - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizing parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tuning the parameters using the entire 2400 takes too long. So, picking randomly 100 points and sampling.\n",
    "# issue - unequale data!\n",
    "\n",
    "some_random_sample = np.random.randint(1,high = len(feature_train), size = 100, )\n",
    "\n",
    "randomized_feature_train_sample = feature_train[some_random_sample]\n",
    "randomized_train_labels_sample = np.array(train_labels.sort_values())[some_random_sample]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  30 out of  30 | elapsed:   51.3s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "56.804641008377075\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameter grid\n",
    "param_grid = {\n",
    "    'n_estimators': np.linspace(10, 200).astype(int),\n",
    "    'max_depth': [None] + list(np.linspace(3, 20).astype(int)),\n",
    "    'max_features': ['auto', 'sqrt', None] + list(np.arange(0.5, 1, 0.1)),\n",
    "    'max_leaf_nodes': [None] + list(np.linspace(10, 50, 500).astype(int)),\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'bootstrap': [True, False]\n",
    "}\n",
    "\n",
    "# Estimator for use in random search\n",
    "estimator = RandomForestClassifier(random_state = RSEED)\n",
    "\n",
    "# Create the random search model\n",
    "rs = RandomizedSearchCV(estimator, param_grid, n_jobs = -1, \n",
    "                        scoring = 'roc_auc', cv = 3, \n",
    "                        n_iter = 10, verbose = 1, random_state=RSEED)\n",
    "\n",
    "# Fit \n",
    "rs.fit(randomized_feature_train_sample, randomized_train_labels_sample)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'n_estimators': 48,\n",
       " 'min_samples_split': 10,\n",
       " 'max_leaf_nodes': 36,\n",
       " 'max_features': 0.6,\n",
       " 'max_depth': 13,\n",
       " 'bootstrap': False}"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# inspecting best parameters:\n",
    "\n",
    "rs.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# retrainning and testing the model using the optimized paramaeters:\n",
    "\n",
    "best_model = rs.best_estimator_\n",
    "\n",
    "train_rf_predictions = best_model.predict(feature_train)\n",
    "train_rf_probs = best_model.predict_proba(feature_train)[:, 1]\n",
    "\n",
    "rf_predictions = best_model.predict(feature_test)\n",
    "rf_probs = best_model.predict_proba(feature_test)[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimized Train ROC AUC Score: 0.5175246010018868\n",
      "Optimized Test ROC AUC  Score: 0.5219338628879976\n"
     ]
    }
   ],
   "source": [
    "print(f'Optimized Train ROC AUC Score: {metrics.roc_auc_score(train_labels, train_rf_probs)}')\n",
    "print(f'Optimized Test ROC AUC  Score: {metrics.roc_auc_score(test_labels, rf_probs)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lower AUC than before - likely because 100 isn't big enough sample to tune."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
